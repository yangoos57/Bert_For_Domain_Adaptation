{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer,Trainer,DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "model = BertForMaskedLM.from_pretrained('beomi/kcbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset('csv',data_files='data/book_train_128.csv')\n",
    "validation = load_dataset('csv',data_files='data/book_validation_128.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-b37cd5b9b58c5c8c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5cfc02813bdf83b5.arrow\n",
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-df0bc971df98e82e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-bf7c412741a4be5e.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sen'], max_length=128, padding=True, truncation=True)\n",
    "\n",
    "train_data_set = train['train'].map(tokenize_function,batch_size=True)\n",
    "validation_data_set = validation['train'].map(tokenize_function,batch_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: Unnamed: 0, sen. If Unnamed: 0, sen are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 175900\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 21988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfe9c65428e4ca0969bc06674597825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0번째 epoch 진행 중 ------- 0번째 step 결과\n",
      "input 문장 : 장 수학 기호 수식에 많이 쓰이는 그리스 알 [MASK] [MASK] [MASK] 읽고 쓰는 법을 배웁니다\n",
      "output 문장 : 장 수학 기호 수식에 많이 쓰이는 그리스 알 [##파] [##벳] [##을]을 쓰는 법을 배웁니다\n",
      "{'loss': 0.0233, 'learning_rate': 4.99886301619065e-05, 'epoch': 0.0}\n",
      "\n",
      "0번째 epoch 진행 중 ------- 5번째 step 결과\n",
      "input 문장 : 물론 이들 언어로 클라이언트 측 [MASK]리케이션을 작성할 [MASK] 있다\n",
      "output 문장 : 물론 이들 언어로 클라이언트 측 [애플]리케이션을 작성할 [수도] 있다\n",
      "{'loss': 0.0166, 'learning_rate': 4.997726032381299e-05, 'epoch': 0.0}\n",
      "\n",
      "0번째 epoch 진행 중 ------- 10번째 step 결과\n",
      "input 문장 : [MASK] 사이 PHP는 Jav [MASK]라고 불리는 Java8처럼 또 최근의 많은 언어가 그러하듯 여러 개발 언어의 좋은 장점이나 [MASK]들을 차용하여 적극 도입하고 개선하여 Modern PHP로 거듭났고 재 [MASK] 중인 회사는 여전히 PHP가 주 [MASK] 개발 언어로 사용 해 [MASK]습니다 .\n",
      "output 문장 : [그] 사이 PHP는 Jav [##a]라고 불리는 Java8처럼 또 최근의 많은 언어가 그러하듯 여러 개발 언어의 좋은 장점이나 [기능]들을 차용하여 적극 사용하고 개선하여 Modven PHP로 거듭났고 재 [##직] 중인 회사는 여전히 PHP가 주 [##력] 개발 언어로 사용 해 [##왔]습니다 .\n",
      "{'loss': 0.765, 'learning_rate': 4.9965890485719485e-05, 'epoch': 0.0}\n",
      "\n",
      "0번째 epoch 진행 중 ------- 15번째 step 결과\n",
      "input 문장 : 아무래도 개념부터 확실히 [MASK]려면 1권부터 읽으면 좋겠다는 생각입니다 .\n",
      "output 문장 : 아무래도 책을 확실히 [세우]려면 1권부터 읽으면 좋겠다는 생각입니다 .\n",
      "{'loss': 1.5284, 'learning_rate': 4.995452064762598e-05, 'epoch': 0.0}\n",
      "\n",
      "0번째 epoch 진행 중 ------- 20번째 step 결과\n",
      "input 문장 : [MASK]이 출간된지 꽤 됬다고 생각하는데 실습하는데 전혀 [MASK]없습니다\n",
      "output 문장 : [책]이 출간된지 꽤 됬다고 생각하는데 실습하는데 전혀 [문제]없습니다\n",
      "{'loss': 1.7636, 'learning_rate': 4.994315080953247e-05, 'epoch': 0.0}\n",
      "\n",
      "0번째 epoch 진행 중 ------- 25번째 step 결과\n",
      "input 문장 : 하드웨어 공격 분석 예제 위협 모델 에서는 [MASK]프 사이클 전체와 최근 [MASK]망 [MASK] 발생하는 하드웨어 [MASK]과 취약성을 다룬다\n",
      "output 문장 : 하드웨어 공격 분석 예제와 모델 에서는 [그래]프 사이클 전체와 최근 [선]망 [##에서] 발생하는 하드웨어 [공격]과 취약성을 다룬다\n",
      "{'loss': 3.4161, 'learning_rate': 4.993178097143897e-05, 'epoch': 0.0}\n",
      "\n",
      "0번째 epoch 진행 중 ------- 30번째 step 결과\n",
      "input 문장 : 또한 정렬 [MASK] 필터 기능이 있는 표를 [MASK] 특정 데이터만 추출하거나 조건에 맞도록 필요한 값 [MASK] 불러올 수 있다 .\n",
      "output 문장 : 또한 정렬 [##과] 필터 기능이 있는 데이터 [이용한] 특정 데이터만 추출하거나 조건에 맞도록 필요한 값 [##을] 불러올 수 있다 .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Bert_For_Fine_Tuning/[tutorial] Bert Domain Adaptation.ipynb 셀 4\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Bert_For_Fine_Tuning/%5Btutorial%5D%20Bert%20Domain%20Adaptation.ipynb#W5sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m (loss, outputs) \u001b[39mif\u001b[39;00m return_outputs \u001b[39melse\u001b[39;00m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Bert_For_Fine_Tuning/%5Btutorial%5D%20Bert%20Domain%20Adaptation.ipynb#W5sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m trainer \u001b[39m=\u001b[39m customtrainer(model\u001b[39m=\u001b[39mmodel, train_dataset\u001b[39m=\u001b[39mtrain_data_set, eval_dataset\u001b[39m=\u001b[39mvalidation_data_set,data_collator\u001b[39m=\u001b[39mdata_collator_BERT,args\u001b[39m=\u001b[39mtraining_args,tokenizer\u001b[39m=\u001b[39mtokenizer,callbacks\u001b[39m=\u001b[39m[myCallback])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Bert_For_Fine_Tuning/%5Btutorial%5D%20Bert%20Domain%20Adaptation.ipynb#W5sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Bert_For_Fine_Tuning/%5Btutorial%5D%20Bert%20Domain%20Adaptation.ipynb#W5sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39m# trainer.train()\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Bert_For_Fine_Tuning/%5Btutorial%5D%20Bert%20Domain%20Adaptation.ipynb#W5sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1408\u001b[0m )\n\u001b[0;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1414\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/trainer.py:1651\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1649\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1650\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1651\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1653\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1655\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1656\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1657\u001b[0m ):\n\u001b[1;32m   1658\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/trainer.py:2363\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2362\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2363\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2365\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, TrainerCallback\n",
    "\n",
    "### Training Arguments\n",
    "device = 'cpu'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    per_device_train_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "data_collator_BERT = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "class myCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A bare :class:`~transformers.TrainerCallback` that just prints the logs.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_step_begin(self, args, state, control, logs=None, **kwargs):\n",
    "\n",
    "        # 함수 이름.. 언제 시작할지\n",
    "        # log는 설정할 때마다\n",
    "        # arg,state,control은 참고할 수 있는 attribute의 경우임.\n",
    "        # 근데 내가 필요한건 input\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            print(\"\")\n",
    "            print(\n",
    "                f\"{int(state.epoch)}번째 epoch 진행 중 ------- {state.global_step}번째 step 결과\"\n",
    "            )\n",
    "\n",
    "\n",
    "class customtrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    ############# 내용 추가\n",
    "    def step_check(self):\n",
    "        # state는 현 상태를 담는 attribute임.\n",
    "        return self.state.global_step\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        ############# 내용 추가\n",
    "        if self.step_check() % self.args.logging_steps == 0:\n",
    "            # step_check = 현 step 파악\n",
    "            # args.logging_steps = argument에서 지정한 step 불러오가\n",
    "\n",
    "            # batch 중 0 번째 위치한 문장 선택\n",
    "            num = 1\n",
    "            input_id = inputs.input_ids[num].reshape(-1).data.tolist()\n",
    "            output_id = outputs.logits[num].argmax(dim=-1).reshape(-1).data.tolist()\n",
    "            attention_mask = inputs.attention_mask[num]\n",
    "\n",
    "            # mask가 위치한 idx 추출하기\n",
    "            mask_idx = (inputs.input_ids[num] == 4).nonzero().data.reshape(-1).tolist()\n",
    "\n",
    "            # padding 제거\n",
    "            input_id_without_pad = [\n",
    "                input_id[i] for i in range(len(input_id)) if attention_mask[i]\n",
    "            ]\n",
    "            output_id_without_pad = [\n",
    "                output_id[i] for i in range(len(output_id)) if attention_mask[i]\n",
    "            ]\n",
    "\n",
    "            # id to token\n",
    "            # [1:-1] [CLS,SEP] 제거\n",
    "            inputs_tokens = self.tokenizer.convert_ids_to_tokens(input_id_without_pad)[\n",
    "                1:-1\n",
    "            ]\n",
    "            outputs_tokens = self.tokenizer.convert_ids_to_tokens(\n",
    "                output_id_without_pad\n",
    "            )[1:-1]\n",
    "\n",
    "            # output mask 부분 표시하기\n",
    "            for i in mask_idx:\n",
    "                # [CLS,SEP 위치 조정]\n",
    "                outputs_tokens[i - 1] = \"[\" + outputs_tokens[i - 1] + \"]\"\n",
    "\n",
    "            inputs_sen = self.tokenizer.convert_tokens_to_string(inputs_tokens)\n",
    "            outputs_sen = self.tokenizer.convert_tokens_to_string(outputs_tokens)\n",
    "\n",
    "            print(f\"input 문장 : {''.join(inputs_sen)}\")\n",
    "            print(f\"output 문장 : {''.join(outputs_sen)}\")\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = customtrainer(\n",
    "    model=model.to(device),\n",
    "    train_dataset=train_data_set,\n",
    "    eval_dataset=validation_data_set,\n",
    "    data_collator=data_collator_BERT,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[myCallback],\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: Unnamed: 0, sen. If Unnamed: 0, sen are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8537d8dc2b6b42c484ddd0cf92844552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 step 결과\n",
      "input 문장 : [MASK] PCI PCIe 디 [MASK]스 드라이버 에서는 PCI와 PCIe PCI델 [MASK] [MASK]ress 버스를 소개하고 이를 사용하는 카드들을 [MASK] 윈도우 드라이버 작성법을 [MASK]하고 있다\n",
      "output 문장 : [장] PCI PCIe 디 [##바이]스 드라이버 에서는 PCI와 PCIe PCI E [##x] [##p]ress 버스를 소개하고 이를 사용하는 카드들을 [위한] 윈도우 드라이버 작성법을 [소개]하고 있다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "{'train_runtime': 2.4029, 'train_samples_per_second': 4.162, 'train_steps_per_second': 2.081, 'train_loss': 2.7520098228706045e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=2.7520098228706045e-06, metrics={'train_runtime': 2.4029, 'train_samples_per_second': 4.162, 'train_steps_per_second': 2.081, 'train_loss': 2.7520098228706045e-06, 'epoch': 5.0})"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class customtrainer(Trainer) :\n",
    "    def __init__(self, *args,**kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    ############# 내용 추가\n",
    "    def step_check(self) :\n",
    "        # state는 현 상태를 담는 attribute임.\n",
    "        return self.state.global_step\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        \n",
    "        ############# 내용 추가\n",
    "        if self.step_check() % self.args.logging_steps == 0:\n",
    "            # step_check = 현 step 파악\n",
    "            # args.logging_steps = argument에서 지정한 step 불러오가\n",
    "\n",
    "            # batch 중 0 번째 위치한 문장 선택\n",
    "            num = 1\n",
    "            input_id = inputs.input_ids[num].reshape(-1).data.tolist()\n",
    "            output_id = outputs.logits[num].argmax(dim=-1).reshape(-1).data.tolist()\n",
    "            attention_mask = inputs.attention_mask[num]\n",
    "\n",
    "            # mask가 위치한 idx 추출하기 \n",
    "            mask_idx = (inputs.input_ids[num] == 4).nonzero().data.reshape(-1).tolist()\n",
    "\n",
    "\n",
    "            # padding 제거\n",
    "            input_id_without_pad = [input_id[i] for i in range(len(input_id)) if attention_mask[i]]\n",
    "            output_id_without_pad = [output_id[i] for i in range(len(output_id)) if attention_mask[i]]\n",
    "\n",
    "            # id to token \n",
    "            # [1:-1] [CLS,SEP] 제거\n",
    "            inputs_tokens = self.tokenizer.convert_ids_to_tokens(input_id_without_pad)[1:-1]\n",
    "            outputs_tokens = self.tokenizer.convert_ids_to_tokens(output_id_without_pad)[1:-1]\n",
    "\n",
    "            # output mask 부분 표시하기\n",
    "            for i in mask_idx :\n",
    "                # [CLS,SEP 위치 조정]\n",
    "                outputs_tokens[i-1] = '[' + outputs_tokens[i-1] + ']'\n",
    "\n",
    "\n",
    "            inputs_sen = self.tokenizer.convert_tokens_to_string(inputs_tokens)\n",
    "            outputs_sen = self.tokenizer.convert_tokens_to_string(outputs_tokens)\n",
    "\n",
    "\n",
    "            print(f\"input 문장 : {''.join(inputs_sen)}\"  )\n",
    "            print(f\"output 문장 : {''.join(outputs_sen)}\"  )\n",
    "            \n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = customtrainer(model=model, train_dataset=train_data_set, eval_dataset=validation_data_set,data_collator=data_collator_BERT,args=training_args,tokenizer=tokenizer)\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (default, Jun 13 2022, 17:35:03) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
